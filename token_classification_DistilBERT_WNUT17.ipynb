{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "951e591f",
   "metadata": {},
   "source": [
    "# Token Classification\n",
    "\n",
    "- Finetune DistilBERT on the WNUT 17 dataset to detect new entities.\n",
    "- Use your finetuned model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e911c934",
   "metadata": {},
   "source": [
    "Token classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e3a7f",
   "metadata": {},
   "source": [
    "!pip install transformers datasets evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b964dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5dce4e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wnut_17\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d02cbce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e82aea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-corporation',\n",
       " 'I-corporation',\n",
       " 'B-creative-work',\n",
       " 'I-creative-work',\n",
       " 'B-group',\n",
       " 'I-group',\n",
       " 'B-location',\n",
       " 'I-location',\n",
       " 'B-person',\n",
       " 'I-person',\n",
       " 'B-product',\n",
       " 'I-product']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc2c070",
   "metadata": {},
   "source": [
    "The letter that prefixes each ner_tag indicates the token position of the entity:\n",
    "\n",
    "- B- indicates the beginning of an entity.\n",
    "- I- indicates a token is contained inside the same entity (for example, the State token is a part of an entity like Empire State Building).\n",
    "- 0 indicates the token doesnâ€™t correspond to any entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28e61909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert/distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5b80c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a215059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "643b04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_wnut = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ce82cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 0\n",
      "tokens ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']\n",
      "ner_tags [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "input_ids [101, 1030, 2703, 17122, 2009, 1005, 1055, 1996, 3193, 2013, 2073, 1045, 1005, 1049, 2542, 2005, 2048, 3134, 1012, 3400, 2110, 2311, 1027, 9686, 2497, 1012, 3492, 2919, 4040, 2182, 2197, 3944, 1012, 102]\n",
      "attention_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "labels [-100, 0, -100, -100, 0, 0, -100, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, -100, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "for k, v in tokenized_wnut[\"train\"][0].items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "980005f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LABEL VERIFICATION ===\n",
      "Original dataset sample:\n",
      "Tokens: ['@paulwalk', 'It', \"'s\", 'the', 'view']\n",
      "NER tags: [0, 0, 0, 0, 0]\n",
      "Label names: ['O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "After tokenization and alignment:\n",
      "Input IDs: [101, 1030, 2703, 17122, 2009, 1005, 1055, 1996, 3193, 2013]\n",
      "Labels: [-100, 0, -100, -100, 0, 0, -100, 0, 0, 0]\n",
      "Label names: ['IGNORE', 'O', 'IGNORE', 'IGNORE', 'O', 'O', 'IGNORE', 'O', 'O', 'O']\n",
      "\n",
      "Dataset features after tokenization:\n",
      "Features: {'id': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-corporation', 'I-corporation', 'B-creative-work', 'I-creative-work', 'B-group', 'I-group', 'B-location', 'I-location', 'B-person', 'I-person', 'B-product', 'I-product'], id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n",
      "Labels field exists: True\n",
      "Sample label shape: 34\n",
      "Sample input_ids shape: 34\n",
      "=== END VERIFICATION ===\n"
     ]
    }
   ],
   "source": [
    "# Let's verify the labels are being passed correctly\n",
    "print(\"=== LABEL VERIFICATION ===\")\n",
    "print(f\"Original dataset sample:\")\n",
    "print(f\"Tokens: {dataset['train'][0]['tokens'][:5]}\")\n",
    "print(f\"NER tags: {dataset['train'][0]['ner_tags'][:5]}\")\n",
    "print(f\"Label names: {[label_list[i] for i in dataset['train'][0]['ner_tags'][:5]]}\")\n",
    "\n",
    "print(f\"\\nAfter tokenization and alignment:\")\n",
    "print(f\"Input IDs: {tokenized_wnut['train'][0]['input_ids'][:10]}\")\n",
    "print(f\"Labels: {tokenized_wnut['train'][0]['labels'][:10]}\")\n",
    "print(f\"Label names: {[label_list[i] if i != -100 else 'IGNORE' for i in tokenized_wnut['train'][0]['labels'][:10]]}\")\n",
    "\n",
    "print(f\"\\nDataset features after tokenization:\")\n",
    "print(f\"Features: {tokenized_wnut['train'].features}\")\n",
    "print(f\"Labels field exists: {'labels' in tokenized_wnut['train'].features}\")\n",
    "print(f\"Sample label shape: {len(tokenized_wnut['train'][0]['labels'])}\")\n",
    "print(f\"Sample input_ids shape: {len(tokenized_wnut['train'][0]['input_ids'])}\")\n",
    "print(\"=== END VERIFICATION ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab8b6e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2ab1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "130a80f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a795f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-corporation\",\n",
    "    2: \"I-corporation\",\n",
    "    3: \"B-creative-work\",\n",
    "    4: \"I-creative-work\",\n",
    "    5: \"B-group\",\n",
    "    6: \"I-group\",\n",
    "    7: \"B-location\",\n",
    "    8: \"I-location\",\n",
    "    9: \"B-person\",\n",
    "    10: \"I-person\",\n",
    "    11: \"B-product\",\n",
    "    12: \"I-product\",\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-corporation\": 1,\n",
    "    \"I-corporation\": 2,\n",
    "    \"B-creative-work\": 3,\n",
    "    \"I-creative-work\": 4,\n",
    "    \"B-group\": 5,\n",
    "    \"I-group\": 6,\n",
    "    \"B-location\": 7,\n",
    "    \"I-location\": 8,\n",
    "    \"B-person\": 9,\n",
    "    \"I-person\": 10,\n",
    "    \"B-product\": 11,\n",
    "    \"I-product\": 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8331521b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint, num_labels=13, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4e03ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1065' max='1065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1065/1065 06:36, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.254779</td>\n",
       "      <td>0.643777</td>\n",
       "      <td>0.538278</td>\n",
       "      <td>0.586319</td>\n",
       "      <td>0.953589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.269519</td>\n",
       "      <td>0.652798</td>\n",
       "      <td>0.544258</td>\n",
       "      <td>0.593607</td>\n",
       "      <td>0.954225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.294628</td>\n",
       "      <td>0.700326</td>\n",
       "      <td>0.514354</td>\n",
       "      <td>0.593103</td>\n",
       "      <td>0.954352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.293110</td>\n",
       "      <td>0.678516</td>\n",
       "      <td>0.525120</td>\n",
       "      <td>0.592043</td>\n",
       "      <td>0.954860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.291374</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.531100</td>\n",
       "      <td>0.591212</td>\n",
       "      <td>0.954288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1065, training_loss=0.020616970767437574, metrics={'train_runtime': 397.9129, 'train_samples_per_second': 42.648, 'train_steps_per_second': 2.676, 'total_flos': 229914027537180.0, 'train_loss': 0.020616970767437574, 'epoch': 5.0})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"token_classification_DistilBERT_WNUT17\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,  # Increased from 2 to 5\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    "    logging_steps=50,  # Add logging\n",
    "    metric_for_best_model=\"f1\",  # Use F1 score for best model selection\n",
    "    greater_is_better=True,\n",
    "    # save_total_limit=2,  # Save only best 2 checkpoints\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_wnut[\"train\"],\n",
    "    eval_dataset=tokenized_wnut[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f0d99d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc175dd728f40e18c33dbec2f0600a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/266M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Swagam/token_classification_DistilBERT_WNUT17/commit/c60896d541fc48fe2518f0fa91bef692a2e7e809', commit_message='End of training', commit_description='', oid='c60896d541fc48fe2518f0fa91bef692a2e7e809', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Swagam/token_classification_DistilBERT_WNUT17', endpoint='https://huggingface.co', repo_type='model', repo_id='Swagam/token_classification_DistilBERT_WNUT17'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93f827bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Golden State Warriors are an American professional basketball team based in San Francisco.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf79ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED INFERENCE WITH BETTER AGGREGATION\n",
    "from transformers import pipeline\n",
    "\n",
    "# Try different aggregation strategies\n",
    "classifier_first = pipeline(\"ner\", \n",
    "                           model=\"Swagam/token_classification_DistilBERT_WNUT17\",\n",
    "                           aggregation_strategy=\"first\")\n",
    "\n",
    "classifier_max = pipeline(\"ner\", \n",
    "                         model=\"Swagam/token_classification_DistilBERT_WNUT17\",\n",
    "                         aggregation_strategy=\"max\")\n",
    "\n",
    "# Test with higher confidence threshold\n",
    "classifier_simple = pipeline(\"ner\", \n",
    "                            model=\"Swagam/token_classification_DistilBERT_WNUT17\",\n",
    "                            aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"The Golden State Warriors are an American professional basketball team based in San Francisco.\"\n",
    "\n",
    "print(\"=== FIRST AGGREGATION ===\")\n",
    "result_first = classifier_first(text)\n",
    "for item in result_first:\n",
    "    print(f\"{item['word']:15} | {item['entity']:15} | {item['score']:.3f}\")\n",
    "\n",
    "print(\"\\n=== MAX AGGREGATION ===\")\n",
    "result_max = classifier_max(text)\n",
    "for item in result_max:\n",
    "    print(f\"{item['word']:15} | {item['entity']:15} | {item['score']:.3f}\")\n",
    "\n",
    "print(\"\\n=== SIMPLE AGGREGATION ===\")\n",
    "result_simple = classifier_simple(text)\n",
    "for item in result_simple:\n",
    "    print(f\"{item['word']:15} | {item['entity']:15} | {item['score']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5dacd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL FILTERING WITH CONFIDENCE THRESHOLD\n",
    "def filter_low_confidence(results, threshold=0.7):\n",
    "    \"\"\"Filter out predictions below confidence threshold\"\"\"\n",
    "    return [item for item in results if item['score'] >= threshold]\n",
    "\n",
    "def filter_common_words(results, common_words={'the', 'a', 'an', 'are', 'is', 'was', 'were', 'and', 'or', 'but'}):\n",
    "    \"\"\"Filter out common words that shouldn't be entities\"\"\"\n",
    "    return [item for item in results if item['word'].lower() not in common_words]\n",
    "\n",
    "# Apply filters to your original results\n",
    "original_results = [\n",
    "    {'entity': 'B-group', 'score': 0.62402034, 'index': 1, 'word': 'the', 'start': 0, 'end': 3},\n",
    "    {'entity': 'B-location', 'score': 0.8082413, 'index': 2, 'word': 'golden', 'start': 4, 'end': 10},\n",
    "    {'entity': 'I-group', 'score': 0.57333845, 'index': 3, 'word': 'state', 'start': 11, 'end': 16},\n",
    "    {'entity': 'I-group', 'score': 0.90311676, 'index': 4, 'word': 'warriors', 'start': 17, 'end': 25},\n",
    "    {'entity': 'I-group', 'score': 0.3521599, 'index': 5, 'word': 'are', 'start': 26, 'end': 29},\n",
    "    {'entity': 'B-group', 'score': 0.44896773, 'index': 7, 'word': 'american', 'start': 33, 'end': 41},\n",
    "    {'entity': 'I-group', 'score': 0.3984678, 'index': 8, 'word': 'professional', 'start': 42, 'end': 54},\n",
    "    {'entity': 'I-group', 'score': 0.3239563, 'index': 9, 'word': 'basketball', 'start': 55, 'end': 65},\n",
    "    {'entity': 'I-group', 'score': 0.48978537, 'index': 10, 'word': 'team', 'start': 66, 'end': 70},\n",
    "    {'entity': 'B-location', 'score': 0.9849925, 'index': 13, 'word': 'san', 'start': 80, 'end': 83},\n",
    "    {'entity': 'I-location', 'score': 0.9699109, 'index': 14, 'word': 'francisco', 'start': 84, 'end': 93}\n",
    "]\n",
    "\n",
    "print(\"=== ORIGINAL RESULTS ===\")\n",
    "for item in original_results:\n",
    "    print(f\"{item['word']:15} | {item['entity']:15} | {item['score']:.3f}\")\n",
    "\n",
    "print(\"\\n=== AFTER CONFIDENCE FILTER (>= 0.7) ===\")\n",
    "filtered_confidence = filter_low_confidence(original_results, 0.7)\n",
    "for item in filtered_confidence:\n",
    "    print(f\"{item['word']:15} | {item['entity']:15} | {item['score']:.3f}\")\n",
    "\n",
    "print(\"\\n=== AFTER COMMON WORDS FILTER ===\")\n",
    "filtered_common = filter_common_words(original_results)\n",
    "for item in filtered_common:\n",
    "    print(f\"{item['word']:15} | {item['entity']:15} | {item['score']:.3f}\")\n",
    "\n",
    "print(\"\\n=== AFTER BOTH FILTERS ===\")\n",
    "filtered_both = filter_common_words(filter_low_confidence(original_results, 0.7))\n",
    "for item in filtered_both:\n",
    "    print(f\"{item['word']:15} | {item['entity']:15} | {item['score']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84986171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90b5759daa94b17b0e2a1fb99b13df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/266M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-group',\n",
       "  'score': 0.62402034,\n",
       "  'index': 1,\n",
       "  'word': 'the',\n",
       "  'start': 0,\n",
       "  'end': 3},\n",
       " {'entity': 'B-location',\n",
       "  'score': 0.8082413,\n",
       "  'index': 2,\n",
       "  'word': 'golden',\n",
       "  'start': 4,\n",
       "  'end': 10},\n",
       " {'entity': 'I-group',\n",
       "  'score': 0.57333845,\n",
       "  'index': 3,\n",
       "  'word': 'state',\n",
       "  'start': 11,\n",
       "  'end': 16},\n",
       " {'entity': 'I-group',\n",
       "  'score': 0.90311676,\n",
       "  'index': 4,\n",
       "  'word': 'warriors',\n",
       "  'start': 17,\n",
       "  'end': 25},\n",
       " {'entity': 'I-group',\n",
       "  'score': 0.3521599,\n",
       "  'index': 5,\n",
       "  'word': 'are',\n",
       "  'start': 26,\n",
       "  'end': 29},\n",
       " {'entity': 'B-group',\n",
       "  'score': 0.44896773,\n",
       "  'index': 7,\n",
       "  'word': 'american',\n",
       "  'start': 33,\n",
       "  'end': 41},\n",
       " {'entity': 'I-group',\n",
       "  'score': 0.3984678,\n",
       "  'index': 8,\n",
       "  'word': 'professional',\n",
       "  'start': 42,\n",
       "  'end': 54},\n",
       " {'entity': 'I-group',\n",
       "  'score': 0.3239563,\n",
       "  'index': 9,\n",
       "  'word': 'basketball',\n",
       "  'start': 55,\n",
       "  'end': 65},\n",
       " {'entity': 'I-group',\n",
       "  'score': 0.48978537,\n",
       "  'index': 10,\n",
       "  'word': 'team',\n",
       "  'start': 66,\n",
       "  'end': 70},\n",
       " {'entity': 'B-location',\n",
       "  'score': 0.9849925,\n",
       "  'index': 13,\n",
       "  'word': 'san',\n",
       "  'start': 80,\n",
       "  'end': 83},\n",
       " {'entity': 'I-location',\n",
       "  'score': 0.9699109,\n",
       "  'index': 14,\n",
       "  'word': 'francisco',\n",
       "  'start': 84,\n",
       "  'end': 93}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"ner\", model=\"Swagam/token_classification_DistilBERT_WNUT17\")\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7875305f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-12-LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
